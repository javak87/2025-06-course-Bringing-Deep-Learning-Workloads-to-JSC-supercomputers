<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Alexandre Strube // Sabrina Benassou // Javad Kasravi">
  <title>Bringing Deep Learning Workloads to JSC supercomputers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./dist/reset.css">
  <link rel="stylesheet" href="./dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="./dist/theme/sky.css" id="theme">
  <style>
  .container{
    display: flex;
  }
  .col {
    flex: 1;
  }

  .slides {
      font-size: 0.75em;
  }
  .reveal ul {
      display: block;
  }
  .reveal ol {
      display: block;
  }

  img {
      max-height: 600px !important;
  }

  figcaption {
      font-size: 0.6em !important;
      font-style: italic !important;
  }

  .subtitle {
      font-style: italic !important;
  }

  .date {
      font-size: 0.75em !important;
  }


  body {
      font-family: "Arial", "sans-serif"
  }

  section {
      margin: 0;
  }

  .reveal .slides {
      margin: 0 1vmin;
  }
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4 {
      font-family: "Arial", "sans-serif";
      text-transform: Uppercase;
      color: #023d6b;
  }

  .reveal h1 {
      color: #023d6b;
      font-size: 250%;
  }


  .reveal h2 + h3 {
      text-transform: Unset;
      font-size: 80%;
  }

  .controls {
      visibility: hidden;
  }

  .reveal .progress {
      position: absolute;
      bottom: 1px;
  }

  .prompt {
      min-width: 0;
      width: 0;
      visibility: hidden;
  }

  div.dateauthor {
      padding-top: 4em;
      color: white;
  }

  div.prompt {
      width:0;
  }


  div#footer {
      position: fixed;
      bottom: 0;
      width: 100%;
      z-index: 10;
  font-size: 0.5em; font-weight: bold; padding: 0 1vmin; height: 20vmin; background: #fff}
  #footer h1 {
      position: absolute; 
      bottom: 3.2vmin; 
      display: block; 
      padding: 0 1em; 
      font-size: 1.7vmin;
      font-weight: bold;
      text-transform: unset;
      color: #023d6b;
  }
  #footer h2 {display: block; padding: 0.em 1em 0;}

  img.fzjlogo {
      position: fixed;
      bottom: 0;
      right: 0;
      height: 24vmin; /* The height of the svg is about 3 times the height of the logo */
      margin-bottom: -3vmin; /* Baseline of logo should be about 5% of short side above edge. */
  }

  .rendered_html img, svg {
      max-height: 440px;
  }

  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bringing Deep Learning Workloads to JSC
supercomputers</h1>
  <p class="subtitle">AI Profiling</p>
  <p class="author">Alexandre Strube // Sabrina Benassou // Javad
Kasravi</p>
  <p class="date">March 25th, 2025</p>
</section>

<section class="slide level2">

<h3 id="performance-terminology">Performance Terminology</h3>
<ul>
<li class="fragment"><p>Latency: the time it takes for one GPU or node
to start exchanging information with another GPU or node.</p></li>
<li class="fragment"><p>Bandwidth: The maximum amount of data that can
be transferred per unit of time between GPUs, CPUs, or nodes.</p></li>
<li class="fragment"><p>Host: CPU + system memory.</p></li>
<li class="fragment"><p>Device: GPU + GPU memory.</p></li>
</ul>
</section>
<section id="base-system" class="slide level2">
<h2>Base system</h2>
<ul>
<li class="fragment"><p>Host</p></li>
<li class="fragment"><p>Devices (4 GPUs)</p></li>
<li class="fragment"><p>Host-Device PCIe bus connection</p></li>
</ul>
<p><img data-src="images/profiling/base_system.png" /></p>
</section>
<section id="naive-communication" class="slide level2">
<h2>Naive communication</h2>
<p><strong>Data Path:</strong></p>
<pre class="mermaid"><code>GPU 0  →  PCI Bus  →  System Memory →  PCI Bus →  GPU 1</code></pre>
<p><img data-src="images/profiling/host_staging_copy.png" /></p>
</section>
<section id="peer-to-peer-communication" class="slide level2">
<h2>Peer-to-Peer communication</h2>
<p><strong>Data Path:</strong></p>
<pre class="mermaid"><code>GPU 0  →  PCI Bus  →  GPU 1</code></pre>
<p><img data-src="images/profiling/p2p-memory-access.png" /></p>
</section>
<section id="nvlink-pcie-bridge-communication" class="slide level2">
<h2>NVLink PCIe Bridge communication</h2>
<p><strong>Data Path (same VLink PCIe Bridge)</strong></p>
<pre class="mermaid"><code>    GPU0[GPU 0] --&gt; Bridge[NVLink PCIe Bridge] --&gt; GPU1[GPU 1]

    classDef gpu fill:#ffcccc,stroke:#333,stroke-width:2px;
    classDef bridge fill:#cce5ff,stroke:#333,stroke-width:2px;

    class GPU0,GPU1 gpu;
    class Bridge bridge;
</code></pre>
<p><img data-src="images/profiling/NVlink_per_node.png" /></p>
</section>
<section id="nvlink-pcie-bridge-communication-1" class="slide level2">
<h2>NVLink PCIe Bridge communication</h2>
<p><strong>Data Path (different VLink PCIe Bridge):</strong></p>
<pre class="mermaid"><code>GPU 0  →  PCI Bus  →  GPU 2</code></pre>
<p><img data-src="images/profiling/NVlinkp2p.png" /></p>
</section>
<section id="nvlink-switch-communication" class="slide level2">
<h2>NVLink Switch communication</h2>
<p><strong>Data Path</strong></p>
<pre class="mermaid"><code>GPU 0  →  NVSwitch  →  GPU 1</code></pre>
<p><img data-src="images/profiling/NVswitch_00.png" /></p>
</section>
<section id="nvlink-switch-communication-1" class="slide level2">
<h2>NVLink Switch communication</h2>
<p><strong>Data Path</strong></p>
<pre class="mermaid"><code>GPU 0  →  NVSwitch  →  GPU 2</code></pre>
<p><img data-src="images/profiling/NVswitch_01.png" /></p>
</section>
<section id="what-this-code-does" class="slide level2">
<h2>What this code does</h2>
<ul>
<li class="fragment">It trains a <a
href="https://arxiv.org/pdf/1706.03762">Transformer</a> language model
on <a
href="https://huggingface.co/datasets/mindchain/wikitext2">WikiText-2</a>
dataset to predict the next word in a sequence.</li>
<li class="fragment"><strong>Transformers</strong> is a deep learning
model architecture that uses self-attention to process sequences in
parallel.</li>
<li class="fragment"><strong>WikiText-2</strong> is a word-level
language modeling dataset consisting of over 2 million tokens extracted
from high-quality Wikipedia articles.</li>
</ul>
</section>
<section id="what-this-code-does-1" class="slide level2">
<h2>What this code does</h2>
<ul>
<li class="fragment"><p>Again, this is not a deep learning
course.</p></li>
<li class="fragment"><p>If you are not familiar with the model and the
dataset, just imagine it as a black box: you provide it with text, and
it generates another text.</p>
<p><img data-src="images/black_box.svg" /></p></li>
</ul>
</section>
<section id="libraries" class="slide level2">
<h2>Libraries</h2>
<ul>
<li class="fragment">You already downloaded the libraries yesterday that
we will use today:
<ul>
<li class="fragment"><strong>PyTorch:</strong> A deep learning framework
for building and training models.</li>
<li class="fragment"><strong>datasets</strong> A library from Hugging
Face used to access, load, process, and share large datasets.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>Let’s have a look at the files
<strong><code>to_distrbuted_training.py</code></strong> and
<strong><code>run_to_distributed_training.sbatch</code></strong> in the
repo.</p>
<p><img data-src="images/look.jpg" /></p>
</section>
<section id="run-the-training-script" class="slide level2">
<h2>Run the Training Script</h2>
<ul>
<li class="fragment"><p>There are TODOs in these two files. <strong>Do
not modify the TODOs for now</strong>. The code is already working, so
you don’t need to make any changes at this point.</p></li>
<li class="fragment"><p>Now run:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_distributed_training.sbatch </span></code></pre></div></li>
<li class="fragment"><p>Spoiler alert 🚨</p></li>
<li class="fragment"><p>The code won’t work.</p></li>
<li class="fragment"><p>Check the output and error files</p></li>
</ul>
</section>
<section id="what-is-the-problem" class="slide level2">
<h2>What is the problem?</h2>
<ul>
<li class="fragment">Remember, there is no internet on the compute
node.</li>
<li class="fragment">Therefore, you should:
<ul>
<li class="fragment"><p><strong>Comment out</strong> lines 76
<strong>to</strong> 135.</p></li>
<li class="fragment"><p>Activate your environment:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> <span class="va">$HOME</span>/course/sc_venv_template/activate.sh</span></code></pre></div></li>
<li class="fragment"><p>Run:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> to_distributed_training.py</span></code></pre></div></li>
<li class="fragment"><p><strong>Uncomment back</strong> lines
76-135.</p></li>
<li class="fragment"><p>Finally, run your job again 🚀:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_distributed_training.sbatch</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="job-running" class="slide level2">
<h2>JOB Running</h2>
<ul>
<li class="fragment">Congrats, you are training a DL model on the
supercomputer using one GPU 🎉</li>
</ul>
</section>
<section id="llview" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment">You can monitor your training using <a
href="https://go.fzj.de/llview-jureca">llview</a>.</li>
<li class="fragment">Use your Judoor credentials to connect.</li>
<li class="fragment">Check the job number that you are intrested in.
<img data-src="images/llview_job.png" height="400" /></li>
</ul>
</section>
<section id="llview-1" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment">Go to the right to open the PDF document.
<strong>It may take some time to load the job information, so please
wait until the icon turns blue</strong>. <img
data-src="images/llview.png" height="450" /></li>
</ul>
</section>
<section id="llview-2" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment">You have many information about your job once you
open the PDF file. <img data-src="images/llview_info.png" /></li>
</ul>
</section>
<section id="gpu-utilization" class="slide level2">
<h2>GPU utilization</h2>
<ul>
<li class="fragment"><p>You can see that in fact we are using <strong>1
GPU</strong></p>
<p><img data-src="images/llview_gpu_1.png" /></p></li>
</ul>
</section>
<section id="gpu-utilization-1" class="slide level2">
<h2>GPU utilization</h2>
<ul>
<li class="fragment"><p>It is a waste of resources.</p></li>
<li class="fragment"><p>The training takes time (1h32m according to
llview).</p></li>
<li class="fragment"><p>Then, can we run our model on multiple GPUs
?</p></li>
</ul>
</section>
<section id="what-if" class="slide level2">
<h2>What if</h2>
<ul>
<li class="fragment"><p>At line 3 in file
<strong><code>run_to_distributed_training.sbatch</code></strong>, we
increase the number of GPUs to 4:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:4</span></span></code></pre></div></li>
<li class="fragment"><p>And run our job again</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_distributed_training.sbatch</span></code></pre></div></li>
</ul>
</section>
<section id="llview-3" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment"><p>We are still using <strong>1
GPU</strong></p></li>
<li class="fragment"><p><img
data-src="images/llview_gpu_2.png" /></p></li>
</ul>
</section>
<section id="we-need-communication" class="slide level2">
<h2>We need communication</h2>
<ul>
<li class="fragment"><p>Without correct setup, the GPUs might not be
utilized.</p></li>
<li class="fragment"><p>Furthermore, we don’t have an established
communication between the GPUs</p>
<p><img data-src="images/dist/no_comm.svg" height="400" /></p></li>
</ul>
</section>
<section id="we-need-communication-1" class="slide level2">
<h2>We need communication</h2>
<p><img data-src="images/dist/comm1.svg" height="500" /></p>
</section>
<section id="we-need-communication-2" class="slide level2">
<h2>We need communication</h2>
<p><img data-src="images/dist/comm2.svg" height="500" /></p>
</section>
<section id="collective-operations" class="slide level2">
<h2>collective operations</h2>
<ul>
<li class="fragment">The GPUs use collective operations to communicate
and share data in parallel computing</li>
<li class="fragment">The most common collective operations are: All
Reduce, All Gather, and Reduce Scatter</li>
</ul>
</section>
<section id="all-reduce" class="slide level2">
<h2>All Reduce</h2>
<p><img data-src="images/dist/all_reduce.svg" /></p>
<ul>
<li class="fragment">Other operations, such as <strong>min</strong>,
<strong>max</strong>, and <strong>avg</strong>, can also be performed
using All-Reduce.</li>
</ul>
</section>
<section id="all-gather" class="slide level2">
<h2>All Gather</h2>
<p><img data-src="images/dist/all_gather.svg" /></p>
</section>
<section id="reduce-scatter" class="slide level2">
<h2>Reduce Scatter</h2>
<p><img data-src="images/dist/reduce_scatter.svg" /></p>
</section>
<section id="terminologies" class="slide level2">
<h2>Terminologies</h2>
<ul>
<li class="fragment">Before going further, we need to learn some
terminologies</li>
</ul>
</section>
<section id="world-size" class="slide level2">
<h2>World Size</h2>
<p><img data-src="images/dist/gpus.svg" height="550" /></p>
</section>
<section id="rank" class="slide level2">
<h2>Rank</h2>
<p><img data-src="images/dist/rank.svg" height="550" /></p>
</section>
<section id="local_rank" class="slide level2">
<h2>local_rank</h2>
<p><img data-src="images/dist/local_rank.svg" height="550" /></p>
</section>
<section id="now" class="slide level2">
<h2>Now</h2>
<p>That we have understood how the devices communicate and the
terminologies used in parallel computing, we can move on to distributed
training (training on multiple GPUs).</p>
</section>
<section id="distributed-training" class="slide level2">
<h2>Distributed Training</h2>
<ul>
<li class="fragment">Parallelize the training across multiple
nodes,</li>
<li class="fragment">Significantly enhancing training speed and model
accuracy.</li>
<li class="fragment">It is particularly beneficial for large models and
computationally intensive tasks, such as deep learning.<a
href="https://pytorch.org/tutorials/distributed/home.html">[1]</a></li>
</ul>
</section>
<section id="distributed-data-parallel-ddp" class="slide level2">
<h2>Distributed Data Parallel (DDP)</h2>
<p><a
href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DDP</a>
is a method in parallel computing used to train deep learning models
across multiple GPUs or nodes efficiently.</p>
<p><img data-src="images/ddp/ddp-2.svg" height="400" /></p>
</section>
<section id="ddp" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-3.svg" height="500" /></p>
</section>
<section id="ddp-1" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-4.svg" height="500" /></p>
</section>
<section id="ddp-2" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-5.svg" height="500" /></p>
</section>
<section id="ddp-3" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-6.svg" height="500" /></p>
</section>
<section id="ddp-4" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-7.svg" height="500" /></p>
</section>
<section id="ddp-5" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-8.svg" height="500" /></p>
</section>
<section id="ddp-6" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/ddp-9.svg" height="500" /></p>
</section>
<section id="ddp-7" class="slide level2">
<h2>DDP</h2>
<p>If you’re scaling DDP to use multiple nodes, the underlying principle
remains the same as single-node multi-GPU training.</p>
</section>
<section id="ddp-8" class="slide level2">
<h2>DDP</h2>
<p><img data-src="images/ddp/multi_node.svg" height="500" /></p>
</section>
<section id="ddp-recap" class="slide level2">
<h2>DDP recap</h2>
<ul>
<li class="fragment">Each GPU on each node gets its own process.</li>
<li class="fragment">Each GPU has a copy of the model.</li>
<li class="fragment">Each GPU has visibility into a subset of the
overall dataset and will only see that subset.</li>
<li class="fragment">Each process performs a full forward and backward
pass in parallel and calculates its gradients.</li>
<li class="fragment">The gradients are synchronized and averaged across
all processes.</li>
<li class="fragment">Each process updates its optimizer.</li>
</ul>
</section>
<section id="lets-start-coding" class="slide level2">
<h2>Let’s start coding!</h2>
<ul>
<li class="fragment"><p>Whenever you see <strong>TODOs</strong>💻📝,
follow the instructions to either copy-paste the code at the specified
line numbers or type it yourself.</p></li>
<li class="fragment"><p>Depending on how you copy and paste, the line
numbers may vary, but always refer to the TODO numbers in the code and
slides.</p></li>
</ul>
</section>
<section id="setup-communication" class="slide level2">
<h2>Setup communication</h2>
<ul>
<li class="fragment">We need to setup a communication among the
GPUs.</li>
<li class="fragment">For that we would need the file
<strong><code>distributed_utils.py</code></strong>.</li>
<li class="fragment"><strong>TODOs</strong>💻📝:
<ol type="1">
<li class="fragment"><p>Import
<strong><code>distributed_utils</code></strong> file at line 11:</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This file contains utility_functions for distributed training.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> distributed_utils <span class="im">import</span> <span class="op">*</span></span></code></pre></div></li>
<li class="fragment"><p>Then <strong>remove</strong> lines 65 and
66:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">## </span><span class="al">TODO</span><span class="co"> 2-3: Remove this line and replace it with a call to the utility function setup().</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span></code></pre></div></li>
<li class="fragment"><p>and <strong>add</strong> at line 65 a call to
the method <strong><code>setup()</code></strong> defined in
<strong><code>distributed_utils.py</code></strong>:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a communication group and return the right identifiers.</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>local_rank, rank, device <span class="op">=</span> setup()</span></code></pre></div></li>
</ol></li>
</ul>
</section>
<section id="setup-communication-1" class="slide level2">
<h2>Setup communication</h2>
<p>What is in the <strong><code>setup()</code></strong> method ?</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup():</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initializes a communication group using &#39;nccl&#39; as the backend for GPU communication.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    torch.distributed.init_process_group(backend<span class="op">=</span><span class="st">&#39;nccl&#39;</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the identifier of each process within a node</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    local_rank <span class="op">=</span> <span class="bu">int</span>(os.getenv(<span class="st">&#39;LOCAL_RANK&#39;</span>))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the global identifier of each process within the distributed system</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    rank <span class="op">=</span> <span class="bu">int</span>(os.environ[<span class="st">&#39;RANK&#39;</span>])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creates a torch.device object that represents the GPU to be used by this process.</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span>, local_rank)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sets the default CUDA device for the current process, </span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ensuring all subsequent CUDA operations are performed on the specified GPU device.</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(device)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Different random seed for each process.</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    torch.random.manual_seed(<span class="dv">1000</span> <span class="op">+</span> torch.distributed.get_rank())</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_rank, rank, device</span></code></pre></div>
</section>
<section id="distributedsampler" class="slide level2">
<h2>DistributedSampler</h2>
<ul>
<li class="fragment"><p><strong>TODO 4</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 76, instantiate a
<strong>DistributedSampler</strong> object for each set to ensure that
each process gets a different subset of the data.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DistributedSampler object for each set to ensure that each process gets a different subset of the data.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>train_sampler <span class="op">=</span> torch.utils.data.distributed.DistributedSampler(train_dataset, </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                                                                shuffle<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                                                                seed<span class="op">=</span>args.seed)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>val_sampler <span class="op">=</span> torch.utils.data.distributed.DistributedSampler(val_dataset)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>test_sampler <span class="op">=</span> torch.utils.data.distributed.DistributedSampler(test_dataset)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="dataloader" class="slide level2">
<h2>DataLoader</h2>
<ul>
<li class="fragment"><p><strong>TODO 5</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 85, <strong>REMOVE</strong>
<strong><code>shuffle=True</code></strong> in the DataLoader of
train_loader and <strong>REPLACE</strong> it by
<strong><code>sampler=train_sampler</code></strong></p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>args.batch_size, </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                        sampler<span class="op">=</span>train_sampler, <span class="co"># pass the sampler argument to the DataLoader</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                        num_workers<span class="op">=</span><span class="bu">int</span>(os.getenv(<span class="st">&#39;SLURM_CPUS_PER_TASK&#39;</span>)),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                        pin_memory<span class="op">=</span><span class="va">True</span>)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="dataloader-1" class="slide level2">
<h2>DataLoader</h2>
<ul>
<li class="fragment"><p><strong>TODO 6</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 90, pass <strong>val_sampler</strong> to
the sampler argument of the val_dataLoader</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset,</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>args.batch_size,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                        sampler<span class="op">=</span>val_sampler, <span class="co"># pass the sampler argument to the DataLoader</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                        pin_memory<span class="op">=</span><span class="va">True</span>)</span></code></pre></div></li>
</ul></li>
<li class="fragment"><p><strong>TODO 7</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 94, pass <strong>test_sampler</strong>
to the sampler argument of the test_dataLoader</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>args.test_batch_size,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                        sampler<span class="op">=</span>test_sampler, <span class="co"># pass the sampler argument to the DataLoader</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                        pin_memory<span class="op">=</span><span class="va">True</span>)    </span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="model" class="slide level2">
<h2>Model</h2>
<ul>
<li class="fragment"><p><strong>TODO 8</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 103, wrap the model in a
<strong>DistributedDataParallel</strong> (DDP) module to parallelize the
training across multiple GPUs.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap the model in DistributedDataParallel module </span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.parallel.DistributedDataParallel(</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    device_ids<span class="op">=</span>[local_rank],</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="sampler" class="slide level2">
<h2>Sampler</h2>
<ul>
<li class="fragment"><p><strong>TODO 9</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At line 117, <strong>set</strong> the current
epoch for the dataset sampler to ensure proper data shuffling in each
epoch</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the current epoch to the sampler to ensure proper data shuffling in each epoch</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>train_sampler.set_epoch(epoch)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="all-reduce-operation" class="slide level2">
<h2>All Reduce Operation</h2>
<ul>
<li class="fragment"><p><strong>TODO 10</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At <strong>lines 37 and 58</strong>, Obtain the
global average loss across the GPUs.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Return the global average loss.</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>torch.distributed.all_reduce(result, torch.distributed.ReduceOp.AVG)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="print" class="slide level2">
<h2>print</h2>
<ul>
<li class="fragment"><p><strong>TODO 11</strong>💻📝:</p>
<ul>
<li class="fragment"><p><strong>Replace</strong> all the
<code>print</code> methods by <strong><code>print0</code></strong>
method defined in <strong><code>distributed_utils.py</code></strong> to
allow only rank 0 to print in the output file.</p></li>
<li class="fragment"><p>At <strong>line 123</strong></p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We use the utility function print0 to print messages only from rank 0.</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>print0(<span class="ss">f&#39;[</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>args<span class="sc">.</span>epochs<span class="sc">}</span><span class="ss">] Train loss: </span><span class="sc">{</span>train_loss<span class="sc">:.5f}</span><span class="ss">, validation loss: </span><span class="sc">{</span>val_loss<span class="sc">:.5f}</span><span class="ss">&#39;</span>)</span></code></pre></div></li>
<li class="fragment"><p>At <strong>line 135</strong></p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We use the utility function print0 to print messages only from rank 0.</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>print0(<span class="st">&#39;Final test loss:&#39;</span>, test_loss.item())</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="print-1" class="slide level2">
<h2>print</h2>
<p>The definition of the function <strong>print0</strong> is in
<strong><code>distributed_utils.py</code></strong></p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>functools.lru_cache(maxsize<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_root_process():</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Return whether this process is the root process.&quot;&quot;&quot;</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.distributed.get_rank() <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print0(<span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Print something only on the root process.&quot;&quot;&quot;</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_root_process():</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span></code></pre></div>
</section>
<section id="save-model" class="slide level2">
<h2>Save model</h2>
<ul>
<li class="fragment"><p><strong>TODO 12</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At <strong>lines 130 and 139</strong>, replace
torch.save method with the utility function save0 to allow only the
process with rank 0 to save the model.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We allow only rank=0 to save the model</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>save0(model, <span class="st">&#39;model-best.pt&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We allow only rank=0 to save the model</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>save0(model, <span class="st">&#39;model-final.pt&#39;</span>)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="save-model-1" class="slide level2">
<h2>Save model</h2>
<p>The method <strong>save0</strong> is defined in
<strong><code>distributed_utils.py</code></strong></p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>functools.lru_cache(maxsize<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_root_process():</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Return whether this process is the root process.&quot;&quot;&quot;</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.distributed.get_rank() <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save0(<span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Pass the given arguments to `torch.save`, but only on the root</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co">    process.</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We do *not* want to write to the same location with multiple</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># processes at the same time.</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_root_process():</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        torch.save(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span></code></pre></div>
</section>
<section id="destroy-process-group" class="slide level2">
<h2>Destroy Process Group</h2>
<ul>
<li class="fragment"><p><strong>TODO 13</strong>💻📝:</p>
<ul>
<li class="fragment"><p>At <strong>line 142</strong>, destroy every
process group and backend by calling destroy_process_group()</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Destroy the process group to clean up resources</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>destroy_process_group()</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="destroy-process-group-1" class="slide level2">
<h2>Destroy Process Group</h2>
<p>The method <strong>destroy_process_group</strong> is defined in
<strong><code>distributed_utils.py</code></strong></p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> destroy_process_group():</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Destroy the process group.&quot;&quot;&quot;</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.distributed.is_initialized():</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        torch.distributed.destroy_process_group()</span></code></pre></div>
</section>
<section id="we-are-almost-there" class="slide level2">
<h2>We are almost there</h2>
<ul>
<li class="fragment">That’s it for the
<strong>to_distributed_training.py</strong> file.</li>
<li class="fragment">But before launching our job, we need to add some
lines to <strong>run_to_distributed_training.sbatch</strong> file</li>
</ul>
</section>
<section id="setup-communication-2" class="slide level2">
<h2>Setup communication</h2>
<p>In <strong><code>run_to_distributed_training.sbatch</code></strong>
file:</p>
<ul>
<li class="fragment"><strong>TODOs 14</strong>💻📝:
<ul>
<li class="fragment"><p>At line 3, increase the number of GPUs to 4 if
it is not already done.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:4</span></span></code></pre></div></li>
<li class="fragment"><p>At line 19, pass the correct number of
devices.</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up four visible GPUs that the job can use </span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span>0,1,2,3</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="setup-communication-3" class="slide level2">
<h2>Setup communication</h2>
<p>Stay in
<strong><code>run_to_distributed_training.sbatch</code></strong>
file:</p>
<ul>
<li class="fragment"><p><strong>TODO 15</strong>💻📝: we need to setup
<strong>MASTER_ADDR</strong> and <strong>MASTER_PORT</strong> to allow
communication over the system.</p>
<ul>
<li class="fragment"><p>At line 22, add the following:</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the first hostname from the list of allocated nodes to use as the master address.</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="va">MASTER_ADDR</span><span class="op">=</span><span class="st">&quot;</span><span class="va">$(</span><span class="ex">scontrol</span> show hostnames <span class="st">&quot;</span><span class="va">$SLURM_JOB_NODELIST</span><span class="st">&quot;</span> <span class="kw">|</span> <span class="fu">head</span> <span class="at">-n</span> 1<span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Modifies the master address to allow communication over InfiniBand cells.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="va">MASTER_ADDR</span><span class="op">=</span><span class="st">&quot;</span><span class="va">${MASTER_ADDR}</span><span class="st">i&quot;</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get IP for hostname.</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">MASTER_ADDR</span><span class="op">=</span><span class="st">&quot;</span><span class="va">$(</span><span class="ex">nslookup</span> <span class="st">&quot;</span><span class="va">$MASTER_ADDR</span><span class="st">&quot;</span> <span class="kw">|</span> <span class="fu">grep</span> <span class="at">-oP</span> <span class="st">&#39;(?&lt;=Address: ).*&#39;</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">MASTER_PORT</span><span class="op">=</span>7010</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="setup-communication-4" class="slide level2">
<h2>Setup communication</h2>
<p>We are not done yet with
<strong><code>run_to_distributed_training.sbatch</code></strong>
file:</p>
<ul>
<li class="fragment"><p><strong>TODO 16</strong>💻📝:</p>
<ul>
<li class="fragment"><p>We <strong>remove</strong> the lauching script
at line 41:</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">srun</span> <span class="at">--cpu_bind</span><span class="op">=</span>none python to_distributed_training.py </span></code></pre></div></li>
<li class="fragment"><p>We use <strong>torchrun_jsc</strong> instead and
pass the following argument:</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch a distributed training job across multiple nodes and GPUs</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="ex">srun</span> <span class="at">--cpu_bind</span><span class="op">=</span>none bash <span class="at">-c</span> <span class="st">&quot;torchrun_jsc </span><span class="dt">\</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="st">    --nnodes=</span><span class="va">$SLURM_NNODES</span><span class="st"> </span><span class="dt">\</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="st">    --rdzv_backend c10d </span><span class="dt">\</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="st">    --nproc_per_node=gpu </span><span class="dt">\</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="st">    --rdzv_id </span><span class="va">$RANDOM</span><span class="st"> </span><span class="dt">\</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="st">    --rdzv_endpoint=</span><span class="va">$MASTER_ADDR</span><span class="st">:</span><span class="va">$MASTER_PORT</span><span class="st"> </span><span class="dt">\</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="st">    --rdzv_conf=is_host=</span><span class="dt">\$</span><span class="st">(if ((SLURM_NODEID)); then echo 0; else echo 1; fi) </span><span class="dt">\</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="st">    to_distributed_training.py &quot;</span></span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="setup-communication-5" class="slide level2">
<h2>Setup communication</h2>
<ul>
<li class="fragment"><p>The arguments that we pass are:</p>
<ol type="1">
<li class="fragment"><strong><code>nnodes=$SLURM_NNODES</code></strong>:
the number of nodes</li>
<li class="fragment"><strong><code>rdzv_backend c10d</code></strong>:
the c10d method for coordinating the setup of communication among
distributed processes.</li>
<li class="fragment"><strong><code>nproc_per_node=gpu</code></strong>
the number of GPUs</li>
<li class="fragment"><strong><code>rdzv_id $RANDOM</code></strong> a
random id which that acts as a central point for initializing and
coordinating the communication among different nodes participating in
the distributed training.</li>
<li
class="fragment"><strong><code>rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT</code></strong>
the IP that we setup in the previous slide to ensure all nodes know
where to connect to start the training session.</li>
<li
class="fragment"><strong><code>rdzv_conf=is_host=\$(if ((SLURM_NODEID)); then echo 0; else echo 1; fi)</code></strong>
The rendezvous host which is responsible for coordinating the initial
setup of communication among the nodes.</li>
</ol></li>
</ul>
</section>
<section id="done" class="slide level2">
<h2>done ✅</h2>
<ul>
<li class="fragment"><p>You can finally run:</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_distributed_training.sbatch</span></code></pre></div></li>
</ul>
</section>
<section id="llview-4" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment"><p>Let’s have a look at our job using <a
href="https://go.fzj.de/llview-jureca">llview</a> again.</p></li>
<li class="fragment"><p>You can see that now, we are using all the GPUs
of the node</p></li>
<li class="fragment"><p><img
data-src="images/llview_gpu_4.png" /></p></li>
</ul>
</section>
<section id="llview-5" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment"><p>And that our job took less time to finish
training (25m vs 1h32m with one GPU)</p></li>
<li class="fragment"><p>But what about using more nodes ?</p></li>
</ul>
</section>
<section id="what-about-using-more-nodes" class="slide level2">
<h2>What about using more nodes ?</h2>
</section>
<section id="multi-node-training" class="slide level2">
<h2>Multi-node training</h2>
<ul>
<li class="fragment"><p><strong>TODO 17</strong>💻📝: in
<strong><code>run_to_distributed_training.sbatch</code></strong> at line
2, you can increase the number of nodes to 2:</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=2</span></span></code></pre></div></li>
<li class="fragment"><p>Hence, you will use 8 GPUs for
training.</p></li>
<li class="fragment"><p>Run again:</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_distributed_training.sbatch</span></code></pre></div></li>
</ul>
</section>
<section id="llview-6" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment"><p>Open <a
href="https://go.fzj.de/llview-jureca">llview</a> again.</p></li>
<li class="fragment"><p>You can see that now, we are using 2 nodes and 8
GPUs.</p></li>
<li class="fragment"><p><img
data-src="images/llview_gpu_8.png" /></p></li>
<li class="fragment"><p>And the training took less time (14m)</p></li>
</ul>
</section>
<section id="amazing" class="slide level2">
<h2>Amazing ✨</h2>
</section>
<section id="before-we-go-further" class="slide level2">
<h2>Before we go further…</h2>
<ul>
<li class="fragment">Distributed Data parallel is usually good enough
👌</li>
<li class="fragment">However, if your model is too big to fit into a
single GPU</li>
<li class="fragment">Welllll … there are other distributed techniques
…</li>
</ul>
</section>
<section id="fully-sharded-data-parallel-fsdp" class="slide level2">
<h2>Fully Sharded Data Parallel (FSDP)</h2>
<p><img data-src="images/fsdp/fsdp-0.svg" style="height:4.16667in" />
<img data-src="images/fsdp_.png" style="height:0.69444in" /></p>
</section>
<section id="fsdp" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-1.svg"
style="height:5.90278in" /></p>
</section>
<section id="fsdp-1" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-2.svg"
style="height:5.90278in" /></p>
</section>
<section id="fsdp-2" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-3.svg"
style="height:5.90278in" /></p>
</section>
<section id="fsdp-3" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-4.svg"
style="height:5.90278in" /></p>
<!-- 
## FSDP

![](images/fsdp/fsdp-5.svg){height=425pt}

---

## FSDP

![](images/fsdp/fsdp-5-5.svg){height=425pt}

---

## FSDP

![](images/fsdp/fsdp-6.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-7.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-8.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-9.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-10.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-11.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-12.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-13.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-14.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-15.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-16.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-17.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-18.svg){height=425pt}

---

## FSDP

![](images/fsdp/fsdp-19.svg){height=425pt}

---

## FSDP

![](images/fsdp/fsdp-20.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-21.svg){height=425pt}



---

## FSDP

![](images/fsdp/fsdp-22.svg){height=425pt}



---

## FSDP

![](images/fsdp/fsdp-23.svg){height=425pt}



---

## FSDP

![](images/fsdp/fsdp-24.svg){height=425pt}



---
 -->
</section>
<section id="fsdp-4" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-25s.svg"
style="height:5.90278in" /></p>
</section>
<section id="fsdp-5" class="slide level2">
<h2>FSDP</h2>
<p><img data-src="images/fsdp/fsdp-28s.svg"
style="height:5.90278in" /></p>
</section>
<section id="fsdp-workflow" class="slide level2">
<h2>FSDP workflow</h2>
<p><img data-src="images/fsdp/fsdp_workflow.svg" /></p>
<!-- 

## FSDP

![](images/fsdp/fsdp-26.svg){height=425pt}



---

## FSDP

![](images/fsdp/fsdp-27.svg){height=425pt}


---

## FSDP

![](images/fsdp/fsdp-28.svg){height=425pt}

--- -->
</section>
<section id="lets-convert-our-ddp-training-code-to-fsdp"
class="slide level2">
<h2>Let’s convert our DDP training Code to FSDP</h2>
</section>
<section id="wrap-the-model-again" class="slide level2">
<h2>Wrap the model AGAIN</h2>
<ul>
<li class="fragment"><p><strong>TODO 17</strong>💻📝:
<strong>Delete</strong> lines 102–107 that wrap the model in
DistributedDataParallel, and instead wrap the model using
torch.distributed.fsdp.</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlike DDP, we should apply fully_shard to both submodules and the root model.</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Here, we apply fully_shard to each TransformerEncoder and TransformerDecoder block,</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and then to the root model.</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>fsdp_kwargs <span class="op">=</span> {}</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> module <span class="kw">in</span> model.modules():</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(module, (</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>            torch.nn.TransformerEncoder, </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>            torch.nn.TransformerDecoder,)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        ):</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each TransformerEncoder and TransformerDecoder block is treated as a separate FSDP unit.</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        torch.distributed.fsdp.fully_shard(module, <span class="op">**</span>fsdp_kwargs)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Identifies all parameters not already wrapped and groups them into a shardable unit.</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>torch.distributed.fsdp.fully_shard(model, <span class="op">**</span>fsdp_kwargs)</span></code></pre></div></li>
</ul>
</section>
<section id="save-model-state" class="slide level2">
<h2>Save Model state</h2>
<ul>
<li class="fragment"><strong>TODO 18</strong>💻📝:
<ul>
<li class="fragment"><p><strong>Remove</strong> lines 137 to 139 and
<strong>replace</strong> them with:</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save sharded model and optimizer</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>save_sharded_model(model, optimizer, <span class="st">&#39;model_best&#39;</span>)</span></code></pre></div></li>
<li class="fragment"><p><strong>Remove</strong> lines 145 to 147 and
<strong>replace</strong> them with:</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save sharded model and optimizer</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>save_sharded_model(model, optimizer, <span class="st">&#39;model_final&#39;</span>)</span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="how-the-model-is-saved" class="slide level2">
<h2>How the model is saved</h2>
<ul>
<li class="fragment"><p>We can either save the full model state, as we
did with DDP, or save the sharded model state. We can also choose to
save the optimizer state.</p></li>
<li class="fragment"><p>The relevant methods can be found in the
<strong>distributed_utils.py</strong> file.</p></li>
<li class="fragment"><p>In both cases, we use <strong>DCP</strong> to
save the model.</p></li>
</ul>
</section>
<section id="what-is-dcp" class="slide level2">
<h2>What is DCP</h2>
<ul>
<li class="fragment"><p>Distributed Checkpoint (DCP) support loading and
saving models from multiple ranks in parallel. It supports load-time
resharding, which means a model can be saved using one cluster
configuration (e.g., number of GPUs or nodes) and later loaded using a
different configuration, without requiring the checkpoint to be
rewritten.</p></li>
<li class="fragment"><p>DCP is different than torch.save and torch.load
in a few significant ways:</p>
<ol type="1">
<li class="fragment">It produces multiple files per checkpoint, with at
least one per rank.</li>
<li class="fragment">It operates in place, meaning that the model should
allocate its data first and DCP uses that storage instead.</li>
</ol></li>
</ul>
</section>
<section id="save-full-model-state" class="slide level2">
<h2>Save full model state</h2>
<ul>
<li class="fragment"><p>We use <strong>get_model_state_dict</strong>
method with <strong>full_state_dict=True</strong> and
<strong>cpu_offload=True</strong> to all-gathers tensors and offload
them to CPU. No ShardedTensor will be in the returned state_dict.</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_full_model(model, optimizer<span class="op">=</span><span class="va">None</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Stream all model parameters to rank 0 on the CPU, then pass all</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co">    other given arguments to `torch.save` to save the model, but only on</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co">    the root process.</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    state_dict_options <span class="op">=</span> dist_state_dict.StateDictOptions(</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        full_state_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        cpu_offload<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    cpu_state_dict <span class="op">=</span> dist_state_dict.get_model_state_dict(</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        options<span class="op">=</span>state_dict_options,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    cpu_state <span class="op">=</span> {<span class="st">&#39;model&#39;</span>: cpu_state_dict}</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        optim_state_dict <span class="op">=</span> dist_state_dict.get_optimizer_state_dict(</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>            model,</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>            optimizer,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>            options<span class="op">=</span>state_dict_options,</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        cpu_state[<span class="st">&#39;optimizer&#39;</span>] <span class="op">=</span> optim_state_dict</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    save0(cpu_state, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span></code></pre></div></li>
</ul>
</section>
<section id="save-sharded-model" class="slide level2">
<h2>Save sharded model</h2>
<ul>
<li class="fragment"><p>We use the <strong>get_model_state_dict</strong>
again, but with <strong>full_state_dict=False</strong> and
<strong>cpu_offload=False</strong>.</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_sharded_model(model, optimizer<span class="op">=</span><span class="va">None</span>, save_dir<span class="op">=</span><span class="st">&#39;checkpoints&#39;</span>):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Obtain sharded model parameters from the GPU, then save the model</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co">    as a distributed checkpoint to the given directory. Saving a</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co">    distributed checkpoint means that the checkpoint will be split into</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">    individual files, one for each process.</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    state_dict_options <span class="op">=</span> dist_state_dict.StateDictOptions(</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>        cpu_offload<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    model_state_dict <span class="op">=</span> dist_state_dict.get_model_state_dict(</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>        options<span class="op">=</span>state_dict_options,</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    cp_state_dict <span class="op">=</span> {<span class="st">&#39;model&#39;</span>: model_state_dict}</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        optim_state_dict <span class="op">=</span> dist_state_dict.get_optimizer_state_dict(</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>            model,</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>            optimizer,</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>            options<span class="op">=</span>state_dict_options,</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        cp_state_dict[<span class="st">&#39;optimizer&#39;</span>] <span class="op">=</span> optim_state_dict</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    dcp.save(</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>        cp_state_dict,</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        storage_writer<span class="op">=</span>dcp.FileSystemWriter(save_dir, overwrite<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div></li>
</ul>
</section>
<section id="run-your-training" class="slide level2">
<h2>Run your training</h2>
<ul>
<li class="fragment"><p>You can run the same sbatch file without any
modification.</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> run_to_fsdp_training.sbatch</span></code></pre></div></li>
</ul>
</section>
<section id="llview-7" class="slide level2">
<h2>llview</h2>
<ul>
<li class="fragment"><p>Let’s have a look at llview again:</p></li>
<li class="fragment"><p><img
data-src="images/llview_fsdp_gpu_8.png" /></p></li>
</ul>
<!-- ---

## sharding_strategy

- **FULL_SHARD**: Parameters, gradients, and optimizer states are sharded. Set **reshard_after_forward=True**
- **SHARD_GRAD_OP**: Similar to PyTorch’s DistributedDataParallel API. Set **reshard_after_forward=False**
- **HYBRID_SHARD**: Apply FULL_SHARD within a node, and replicate parameters across nodes. Set **reshard_after_forward=True** with a 2D device mesh
- **_HYBRID_SHARD_ZERO2**: Apply SHARD_GRAD_OP within a node, and replicate parameters across nodes. This is like HYBRID_SHARD, except this may provide even higher throughput since the unsharded parameters are not freed after the forward pass, saving the all-gathers in the pre-backward. Set **reshard_after_forward=False** with a 2D device mesh -->
</section>
<section id="fsdp-6" class="slide level2">
<h2>FSDP</h2>
<ul>
<li class="fragment">FSDP is a built-in primitive in PyTorch for
distributed training.</li>
<li class="fragment">It is highly memory efficient because it shards
model parameters, gradients, and optimizer states across GPUs.</li>
<li class="fragment">This allows training of very large models (often
&gt;1B parameters) that wouldn’t fit in memory otherwise.</li>
<li class="fragment">However, FSDP relies on frequent communication
between GPUs, so it requires a high-bandwidth interconnect (e.g.,
InfiniBand).</li>
<li class="fragment">On bandwidth-limited clusters, FSDP may become a
bottleneck, and pipeline parallelism might be preferable.</li>
</ul>
</section>
<section
id="thats-it-for-fsdp-now-lets-move-to-another-parallelization-technique."
class="slide level2">
<h2>That’s it for FSDP, now let’s move to another parallelization
technique.</h2>
</section>
<section id="model-parallel" class="slide level2">
<h2>Model Parallel</h2>
<ul>
<li class="fragment">Before talking about pipelining, let’s talk about
Model Parallelism (MP).</li>
<li class="fragment">Model <em>itself</em> is too big to fit in one
single GPU 🐋</li>
<li class="fragment">Each GPU holds a slice of the model 🍕</li>
<li class="fragment">Data moves from one GPU to the next</li>
</ul>
</section>
<section id="model-parallel-1" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel.svg" /></p>
</section>
<section id="model-parallel-2" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-1.svg" /></p>
</section>
<section id="model-parallel-3" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-2.svg" /></p>
</section>
<section id="model-parallel-4" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-3.svg" /></p>
</section>
<section id="model-parallel-5" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-4.svg" /></p>
</section>
<section id="model-parallel-6" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-5.svg" /></p>
</section>
<section id="model-parallel-7" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-6.svg" /></p>
</section>
<section id="model-parallel-8" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-7.svg" /></p>
</section>
<section id="model-parallel-9" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-8.svg" /></p>
</section>
<section id="model-parallel-10" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-9.svg" /></p>
</section>
<section id="model-parallel-11" class="slide level2">
<h2>Model Parallel</h2>
<p><img data-src="images/model-parallel-pipeline-10.svg" /></p>
</section>
<section id="whats-the-problem-here" class="slide level2">
<h2>What’s the problem here? 🧐</h2>
</section>
<section id="model-parallel-12" class="slide level2">
<h2>Model Parallel</h2>
<ul>
<li class="fragment">Waste of resources</li>
<li class="fragment">While one GPU is working, others are waiting the
whole process to end</li>
<li class="fragment"><img data-src="images/no_pipe.png" />
<ul>
<li class="fragment"><a href="https://arxiv.org/abs/1811.06965">Source:
GPipe: Efficient Training of Giant Neural Networks using Pipeline
Parallelism</a></li>
</ul></li>
</ul>
</section>
<section id="model-parallel---pipelining" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img data-src="images/model-parallel-pipeline-1.svg" /></p>
</section>
<section id="model-parallel---pipelining-1" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-2-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-2" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-3-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-3" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-4-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-4" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-5-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-5" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-6-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-6" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-7-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-7" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-8-multibatch.svg" /></p>
</section>
<section id="model-parallel---pipelining-8" class="slide level2">
<h2>Model Parallel - Pipelining</h2>
<p><img
data-src="images/model-parallel-pipeline-9-multibatch.svg" /></p>
</section>
<section id="this-is-an-oversimplification" class="slide level2">
<h2>This is an oversimplification!</h2>
<ul>
<li class="fragment">Actually, you split the input minibatch into
multiple microbatches.</li>
<li class="fragment">There’s still idle time - an unavoidable “bubble”
🫧</li>
<li class="fragment"><img data-src="images/pipe.png" /></li>
</ul>
</section>
<section id="model-parallel---multi-node" class="slide level2">
<h2>Model Parallel - Multi Node</h2>
<ul>
<li class="fragment">In this case, each node does the same as the
others.</li>
<li class="fragment">At each step, they all synchronize their
weights.</li>
</ul>
</section>
<section id="model-parallel---multi-node-1" class="slide level2">
<h2>Model Parallel - Multi Node</h2>
<p><img data-src="images/model-parallel-multi-node.svg" /></p>
</section>
<section id="pipeline-parallelism" class="slide level2">
<h2>Pipeline Parallelism</h2>
<ul>
<li class="fragment">Pipeline parallelism does not require frequent
communication because the model is stored sequentially in stages.</li>
<li class="fragment">If your model is computationally intensive with
extremely wide layers, you may consider Tensor Parallelism (TP).</li>
</ul>
</section>
<section id="tensor-parallelism-tp" class="slide level2">
<h2>Tensor Parallelism (TP)</h2>
<p><img data-src="images/tp/tp-1.png" /></p>
</section>
<section id="tp" class="slide level2">
<h2>TP</h2>
<p><img data-src="images/tp/tp-2.png" /></p>
</section>
<section id="tp-1" class="slide level2">
<h2>TP</h2>
<p><img data-src="images/tp/tp-3.png" /></p>
</section>
<section id="tp-2" class="slide level2">
<h2>TP</h2>
<p><img data-src="images/tp/tp-4.png" /></p>
</section>
<section id="tp-3" class="slide level2">
<h2>TP</h2>
<p><img data-src="images/tp/tp-5.png" /></p>
</section>
<section id="tp-4" class="slide level2">
<h2>TP</h2>
<ul>
<li class="fragment">We have introduced row parallelism.</li>
<li class="fragment">There is also column parallelism, where the weight
columns are split across GPUs.</li>
<li class="fragment">Tensor Parallelism (TP) is great for large,
compute-heavy layers like matrix multiplications.</li>
<li class="fragment">However, TP requires frequent communication during
tensor operations.</li>
</ul>
</section>
<section id="d-parallelism" class="slide level2">
<h2>3D Parallelism</h2>
<figure>
<img data-src="images/3dp.png" alt="3D Parallelism" />
<figcaption aria-hidden="true"><a
href="https://arxiv.org/pdf/2410.06511">3D Parallelism</a></figcaption>
</figure>
<ul>
<li class="fragment">3D Parallelism combines Tensor Parallelism (TP),
Pipeline Parallelism (PP), and Data Parallelism (DP) to efficiently
train large models by distributing computation, memory, and data across
multiple GPUs.</li>
<li class="fragment">It enables scaling to very large models by
addressing compute, memory, and communication bottlenecks in a balanced
way.</li>
</ul>
</section>
<section id="day-2-recap" class="slide level2">
<h2>Day 2 RECAP</h2>
<ul>
<li class="fragment">You know where to store your code and your data.
🗂️📄</li>
<li class="fragment">You know what distributed training is. 🧑‍💻</li>
<li class="fragment">You can submit training jobs on a single GPU,
multiple GPUs, or across multiple nodes. 🎮💻</li>
<li class="fragment">You are familiar with DDP and aware of other
distributed training techniques like FSDP, TP, PP, and 3D parallelism.
⚙️💡</li>
<li class="fragment">You know how to monitor your training using llview.
📊👀</li>
</ul>
</section>
<section id="find-out-more" class="slide level2">
<h2>Find Out More</h2>
<ul>
<li class="fragment"><p>Here are some useful:</p>
<ul>
<li class="fragment">Papers:
<ul>
<li class="fragment"><a href="https://arxiv.org/pdf/2304.11277">FSDP
paper</a></li>
<li class="fragment"><a href="https://arxiv.org/pdf/1811.06965">Pipeline
Parallelism</a></li>
<li class="fragment"><a href="https://arxiv.org/pdf/1909.08053">Tensor
Parallelism</a></li>
</ul></li>
<li class="fragment">Tutorials:
<ul>
<li class="fragment"><a
href="https://sdlaml.pages.jsc.fz-juelich.de/ai/recipes/pytorch_at_jsc/">PyTorch
at JSC</a></li>
<li class="fragment"><a
href="https://github.com/pytorch/tutorials/tree/main">PyTorch tutorials
GitHub</a></li>
<li class="fragment"><a
href="https://pytorch.org/tutorials/distributed/home.html">PyTorch
documentation</a></li>
</ul></li>
<li class="fragment">Links
<ul>
<li class="fragment"><a
href="https://sdlaml.pages.jsc.fz-juelich.de/ai/">AI Landing
Page</a></li>
<li class="fragment"><a
href="https://www.fz-juelich.de/en/ias/jsc/education/training-courses">Other
courses at JSC</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="any-questions" class="slide level2">
<h2>ANY QUESTIONS??</h2>
<h4 id="feedback-is-more-than-welcome">Feedback is more than
welcome!</h4>
</section>
<section class="slide level2">

</section>
    </div>
  </div>

  <script src="./dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="./plugin/notes/notes.js"></script>
  <script src="./plugin/search/search.js"></script>
  <script src="./plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
